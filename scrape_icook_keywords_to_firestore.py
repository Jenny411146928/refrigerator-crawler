# -*- coding: utf-8 -*-
"""
scrape_icook_keywords_to_firestore.py
- é‡å°å¸¸è¦‹é£Ÿæé—œéµå­—ï¼Œåœ¨ iCook æœå°‹é æŠ“å–å‰ N ç­†é£Ÿè­œ
- è§£æ JSON-LD å–å¾—ï¼šæ¨™é¡Œã€ä¸»åœ–ã€é£Ÿæã€æ­¥é©Ÿ(ç´”æ–‡å­—)ã€æ™‚é–“ã€å¹¾äººä»½ã€é€£çµ
- å„²å­˜æœ¬åœ° JSONã€ç´¯ç©æ­·å²ã€ä¸¦å˜—è©¦å¯«å…¥ Firestoreï¼ˆå„ªå…ˆå¾ GitHub Secret è®€é‡‘é‘°ï¼‰
"""

import os, re, json, time, random
from datetime import datetime
from typing import List, Tuple, Optional, Any
from urllib.parse import urljoin, quote

# ---------- è¨­å®š ----------
LOG_FILE = "crawler.log"
SAMPLE_FILE = "icook_keywords_sample.json"
HISTORY_FILE = "icook_keywords_history.json"
LIMIT_PER_ING = 20    # æ¯å€‹é—œéµå­—æŠ“å¹¾ç­†
RATE_DELAY = 1.2      # åŸºæœ¬ç­‰å¾…ï¼ˆç§’ï¼‰

COMMON_INGREDIENTS = [
    "é›è‚‰","è±¬è‚‰","ç‰›è‚‰","ç¾Šè‚‰",
    "é­š","è¦","è›¤èœŠ","èŠ±æ",
    "é«˜éº—èœ","é’æ±Ÿèœ","è èœ","ç©ºå¿ƒèœ",
    "æ´‹è”¥","é’æ¤’","èƒ¡è˜¿è””","ç•ªèŒ„",
    "é¦™è‡","æé®‘è‡","é‡‘é‡è‡",
    "é›è›‹","è±†è…","ç™½é£¯","éºµæ¢"
]

# ---------- Log ----------
def log(msg: str):
    text = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}"
    print(text)
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(text + "\n")
    except Exception:
        # è‹¥å¯« log å¤±æ•—ä¹Ÿä¸è¦ä¸­æ–·
        pass

# ---------- Firestore åˆå§‹åŒ–ï¼šå…ˆå¾ç’°å¢ƒè®Šæ•¸ SERVICE_ACCOUNT_KEYï¼ˆGitHub Secretï¼‰è®€ï¼Œå† fallback æœ¬åœ°æª” ----------
db = None
key_json = os.environ.get("SERVICE_ACCOUNT_KEY")
if key_json:
    try:
        import firebase_admin
        from firebase_admin import credentials, firestore
        creds_dict = json.loads(key_json)
        if not firebase_admin._apps:
            cred = credentials.Certificate(creds_dict)
            firebase_admin.initialize_app(cred)
        db = firestore.client()
        log("âœ… å·²é€£ç·š Firestore (from SERVICE_ACCOUNT_KEY)")
    except Exception as e:
        log(f"âš ï¸ Firestore åˆå§‹åŒ– (from secret) å¤±æ•—ï¼š{e}")
else:
    KEY_FILE = "serviceAccountKey.json"
    if os.path.exists(KEY_FILE):
        try:
            import firebase_admin
            from firebase_admin import credentials, firestore
            if not firebase_admin._apps:
                cred = credentials.Certificate(KEY_FILE)
                firebase_admin.initialize_app(cred)
            db = firestore.client()
            log("âœ… å·²é€£ç·š Firestore (from local file)")
        except Exception as e:
            log(f"âš ï¸ Firestore åˆå§‹åŒ– (from local file) å¤±æ•—ï¼š{e}")
    else:
        log("â„¹ï¸ æœªæ‰¾åˆ° SERVICE_ACCOUNT_KEY èˆ‡æœ¬åœ°é‡‘é‘°ï¼Œåƒ…è¼¸å‡ºåˆ°æœ¬åœ° JSON")

# ---------- Playwright (æœƒåœ¨ runtime import) ----------
from playwright.sync_api import sync_playwright

# ---------- å·¥å…·å‡½å¼ ----------
def upsert_firestore(doc: dict):
    assert "link" in doc
    doc_id = doc["link"].replace("https://", "").replace("http://", "").replace("/", "_")
    db.collection("recipes").document(doc_id).set(doc)
    return doc_id

def iso8601_duration_to_text(s: str) -> Optional[str]:
    if not s: return None
    if not s.startswith("PT"): return s
    hours = minutes = 0
    m_h = re.search(r"PT(\d+)H", s)
    m_m = re.search(r"(\d+)M", s)
    if m_h: hours = int(m_h.group(1))
    if m_m: minutes = int(m_m.group(1))
    if hours == 0 and minutes == 0: return None
    if hours and minutes: return f"{hours} å°æ™‚ {minutes} åˆ†é˜"
    if hours: return f"{hours} å°æ™‚"
    return f"{minutes} åˆ†é˜"

def ensure_str(x: Any) -> Optional[str]:
    return str(x).strip() if x is not None else None

def extract_image_url_from_ld(obj: Any) -> Optional[str]:
    if not obj: return None
    if isinstance(obj, str): return obj
    if isinstance(obj, list) and obj:
        if isinstance(obj[0], str): return obj[0]
        if isinstance(obj[0], dict): return ensure_str(obj[0].get("url"))
    if isinstance(obj, dict): return ensure_str(obj.get("url"))
    return None

def extract_steps_from_ld(obj: Any) -> List[str]:
    steps: List[str] = []
    def pick_from_list(items: List[Any]):
        for it in items:
            if isinstance(it, str):
                t = it.strip()
                if t: steps.append(t)
            elif isinstance(it, dict):
                t = it.get("text") or it.get("name")
                if t and str(t).strip():
                    steps.append(str(t).strip())
    if isinstance(obj, list): pick_from_list(obj)
    elif isinstance(obj, dict):
        if obj.get("@type") == "HowToSection" and isinstance(obj.get("itemListElement"), list):
            pick_from_list(obj["itemListElement"])
        else:
            t = obj.get("text") or obj.get("name")
            if t and str(t).strip():
                steps.append(str(t).strip())
    return [s for s in (s.strip() for s in steps) if s]

def parse_ld_json(ld_texts: List[str]) -> Tuple[Optional[str], List[str], Optional[str], Optional[str], List[str], Optional[str]]:
    title = None
    ingredients: List[str] = []
    time_text = None
    yield_info = None
    steps: List[str] = []
    image_url = None

    for raw in ld_texts:
        try:
            data = json.loads(raw)
        except Exception:
            continue

        candidates = data if isinstance(data, list) else [data]
        for obj in candidates:
            nodes: List[dict] = []
            if isinstance(obj, dict):
                if "@graph" in obj and isinstance(obj["@graph"], list):
                    nodes.extend([g for g in obj["@graph"] if isinstance(g, dict)])
                else:
                    nodes.append(obj)
            elif isinstance(obj, list):
                nodes.extend([x for x in obj if isinstance(x, dict)])
            else:
                continue

            for node in nodes:
                types = node.get("@type")
                is_recipe = (types == "Recipe") or (isinstance(types, list) and "Recipe" in types)
                if not is_recipe:
                    continue

                if not title:
                    title = ensure_str(node.get("name"))

                if not ingredients:
                    ing = node.get("recipeIngredient")
                    if isinstance(ing, list):
                        ingredients = [str(x).strip() for x in ing if str(x).strip()]

                if not time_text:
                    t = ensure_str(node.get("totalTime")) or ensure_str(node.get("cookTime"))
                    if t:
                        time_text = iso8601_duration_to_text(t) or t

                if not yield_info:
                    y = node.get("recipeYield")
                    if isinstance(y, list) and y:
                        yield_info = ensure_str(y[0])
                    else:
                        yield_info = ensure_str(y)

                if not steps:
                    inst = node.get("recipeInstructions")
                    st = extract_steps_from_ld(inst)
                    if st:
                        steps = st

                if not image_url:
                    image_url = extract_image_url_from_ld(node.get("image"))

    return title, ingredients, time_text, yield_info, steps, image_url

# ---------- æŠ“å–®ä¸€é—œéµå­—çš„é‚è¼¯ ----------
def scrape_keyword(keyword: str, page) -> List[dict]:
    search_url = f"https://icook.tw/recipes/search?q={quote(keyword)}"
    log(f"ğŸ” æœå°‹é—œéµå­—ï¼š{keyword} -> {search_url}")
    page.goto(search_url, wait_until="domcontentloaded", timeout=60000)
    page.wait_for_timeout(1500)

    anchors = page.locator("a[href^='/recipes/']").all()
    links = []
    for a in anchors:
        href = a.get_attribute("href")
        if href and re.fullmatch(r"/recipes/\d+", href):
            links.append(urljoin("https://icook.tw", href))
    links = list(dict.fromkeys(links))[:LIMIT_PER_ING]
    log(f"ğŸ‘‰ æ‰¾åˆ° {len(links)} ç­†")

    saved: List[dict] = []
    for i, url in enumerate(links, 1):
        log(f"  [{i}/{len(links)}] æŠ“å–ï¼š{url}")
        try:
            page.goto(url, wait_until="domcontentloaded", timeout=60000)
            page.wait_for_timeout(800)
            scripts = page.locator("script[type='application/ld+json']").all()
        except Exception as e:
            log(f"  âš ï¸ é–‹å•Ÿé é¢å¤±æ•—ï¼š{e}")
            continue

        ld_texts = []
        for s in scripts:
            try:
                txt = s.inner_text()
                if txt and "Recipe" in txt: ld_texts.append(txt)
            except Exception:
                pass

        title, ingredients, time_text, yield_info, steps, image_url = parse_ld_json(ld_texts)

        # fallback: title / image / steps / ingredients
        if not title:
            try:
                title = page.locator("h1").first.inner_text().strip()
            except Exception:
                title = "(æœªæ‰¾åˆ°æ¨™é¡Œ)"
        if not image_url:
            try:
                og = page.locator("meta[property='og:image']").first.get_attribute("content")
                if og and og.startswith("http"):
                    image_url = og
            except Exception:
                pass
        if not steps:
            try:
                candidates = page.locator("li[class*=step], .step, [class*=instruction] li").all()
                tmp = [(c.inner_text() or "").strip() for c in candidates if (c.inner_text() or "").strip()]
                if tmp:
                    steps = tmp
            except Exception:
                pass
        if not ingredients:
            try:
                items = page.locator("li[class*='ingredient'], .ingredients li, [data-ingredient-name]").all()
                tmp = [(it.inner_text() or "").strip() for it in items if (it.inner_text() or "").strip()]
                if tmp:
                    ingredients = tmp
            except Exception:
                pass

        doc = {
            "title": title or "",
            "ingredients": ingredients,
            "steps": steps,
            "time": time_text,
            "yield": yield_info,
            "link": url,
            "imageUrl": image_url,
            "source": "icook"
        }

        if ingredients:
            if db:
                try:
                    doc_id = upsert_firestore(doc)
                    log(f"ğŸ“ å·²å¯«å…¥ Firestoreï¼šrecipes/{doc_id}")
                except Exception as e:
                    log(f"âš ï¸ å¯«å…¥ Firestore å¤±æ•—ï¼š{e}")
            saved.append(doc)
        else:
            log("  âš ï¸ æ­¤é æœªæŠ“åˆ°é£Ÿæï¼Œç•¥é")

        # polite delay + small random jitter between requests
        page.wait_for_timeout(int(RATE_DELAY * 1000 + random.uniform(200, 800)))
    return saved

# ---------- ä¸»ç¨‹å¼ ----------
def main():
    # start jitterï¼šé¿å…æ‰€æœ‰æ’ç¨‹å®Œå…¨åŒæ™‚ç™¼èµ·
    start_jitter = random.uniform(5, 45)  # seconds
    log(f"â± èµ·å§‹éš¨æ©Ÿå»¶é² {start_jitter:.1f} ç§’ä»¥é™ä½è¢«åµæ¸¬é¢¨éšª")
    time.sleep(start_jitter)

    log("ğŸš€ çˆ¬èŸ²é–‹å§‹åŸ·è¡Œ")
    all_saved: List[dict] = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        ctx = browser.new_context(
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/124.0.0.0 Safari/537.36"),
            locale="zh-TW",
        )
        page = ctx.new_page()

        # å¯é€²ä¸€æ­¥æŠŠ COMMON_INGREDIENTS éš¨æ©Ÿæ‰“æ•£ï¼Œæ¸›å°‘å›ºå®šè¡Œç‚ºç‰¹å¾µ
        kws = COMMON_INGREDIENTS.copy()
        random.shuffle(kws)
        for kw in kws:
            docs = scrape_keyword(kw, page)
            all_saved.extend(docs)
            # æ¯æŠ“å®Œä¸€å€‹ keywordï¼ŒåŠ å€‹è¼ƒé•·çš„éš¨æ©Ÿç­‰å¾…ï¼Œé™ä½é »ç‡ç‰¹å¾µ
            sec = random.uniform(2.0, 6.0)
            log(f"  âœ‹ keyword é–“éš™ç­‰å¾… {sec:.1f} ç§’")
            time.sleep(sec)

        ctx.close()
        browser.close()

    # è¼¸å‡º sample jsonï¼ˆè¦†è“‹ï¼‰
    try:
        with open(SAMPLE_FILE, "w", encoding="utf-8") as f:
            json.dump(all_saved, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log(f"âš ï¸ å¯«å…¥ {SAMPLE_FILE} å¤±æ•—ï¼š{e}")

    # ç´¯ç©å¯«æ­·å²
    history = []
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                history = json.load(f)
        except Exception:
            history = []
    history.extend(all_saved)
    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log(f"âš ï¸ å¯«å…¥ {HISTORY_FILE} å¤±æ•—ï¼š{e}")

    log(f"âœ… å®Œæˆï¼æœ¬æ¬¡è™•ç† {len(all_saved)} ç­†ï¼›å·²è¼¸å‡º {SAMPLE_FILE} ä¸¦ç´¯ç©åˆ° {HISTORY_FILE}")

    # Firestore çµ±è¨ˆ & ç´€éŒ„
    if db:
        try:
            docs = db.collection("recipes").stream()
            total = sum(1 for _ in docs)
            log(f"ğŸ“Š Firestore ç›®å‰ recipes ç¸½ç­†æ•¸ï¼š{total}")
            # æ–°å¢ crawler_logs ç´€éŒ„
            log_doc = {
                "timestamp": datetime.now().isoformat(timespec="seconds"),
                "status": "success",
                "count": len(all_saved),
                "recipes_total": total
            }
            db.collection("crawler_logs").add(log_doc)
            log("ğŸ“ å·²å¯«å…¥ crawler_logs")
        except Exception as e:
            log(f"âš ï¸ Firestore çµ±è¨ˆæˆ–ç´€éŒ„å¤±æ•—ï¼š{e}")

    log("ğŸ çˆ¬èŸ²çµæŸ")

if __name__ == "__main__":
    main()

